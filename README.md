# Boosting_GBDT

## Boosting:
1. Boosting is a machine learning ensemble technique that combines multiple weak learners into a strong one.
2. The process involves training a sequence of models, where each model focuses on correcting the errors of its predecessors.
3. The final prediction is a weighted sum of the individual models' predictions.
4. Boosting algorithms, like AdaBoost and Gradient Boosting, enhance model accuracy and generalize well by iteratively improving the model's weaknesses.

## Gradient Boosting (GBDT):
1. Gradient Boosting is an ensemble learning technique used for supervised machine learning tasks, primarily regression and classification.
2. It builds a strong predictive model by combining the outputs of multiple weak learners (usually decision trees) in an iterative manner.
3. GBDT minimizes the loss function by fitting subsequent models to the residuals of the previous ones, gradually improving prediction accuracy.

## XGBoost (Extreme Gradient Boosting) Classifier:
1. XGBoost is an advanced and highly efficient implementation of the Gradient Boosting framework.
2. It incorporates regularization techniques, handling missing values, and optimizing computation for better performance.
3. XGBoost utilizes a combination of gradient boosting and decision trees, making it a popular choice for various tasks due to its speed and superior predictive power.

